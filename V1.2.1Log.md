 # PulseBench V1.2.1 (beta) — Release Notes

 Release date: 2025-11-19

 This is a small beta maintenance release that follows V1.2.0. It focuses on producing
 more friendly compact scores, minor stability fixes, and clearer documentation for
 using the `pulsebench` binary and produced result files.

 Summary of changes
 - Tweak compact scoring: improved log-based mapping for more stable, human-friendly scores.
 - Include `score` consistently in JSON and CSV outputs (compact, low-thousands range).
 - Added example usage snippets for the `pulsebench` binary and explained the meaning of
   key result fields.
 - Minor robustness fixes to stats collection and JSON export.

 Why this release

 PulseBench V1.2.0 introduced JSON/CSV exports, presets and a compact score. Users
 requested a more stable, easy-to-read score (not a huge cumulative number). V1.2.1
 refines the mapping and clarifies how to use the binary and interpret outputs.

 Files changed (high level)
 - `src/main.cpp` — print `Score: <compact>` and include `score` in JSON/CSV exports.
 - `src/benchmark.cpp` — produce the compact score for the internal duration benchmark path.
 - Documentation files — added examples and improved release notes.

 Compact score (what it means)

 PulseBench maps measured throughput (batches/sec) to a compact score intended for
 quick comparison between machines. The mapping is log-based so that small improvements
 in fast machines do not produce extremely large jumps in the numeric score. Typical
 values are in the low thousands and are easier to communicate (for example: 1200, 2100, 3050).

 Using the `pulsebench` binary

 Below are common ways to run the binary that ship with this release. Replace `./pulsebench`
 with the path to your built binary if different.

 - List workloads

 ```bash
 ./pulsebench --list
 ```

 - Run a workload by name (default `simd`), 10 seconds, auto threads:

 ```bash
 ./pulsebench --workload simd --duration 10
 ```

 - Run with a custom thread count and workset size (128MiB workset):

 ```bash
 ./pulsebench --workload memcpy --duration 10 --threads 4 --workset 134217728
 ```

 - Run and write rich JSON output (includes `stats`, `percentiles`, `histogram_bins`, `score`):

 ```bash
 ./pulsebench --workload simd --duration 10 --threads 4 --output results.json --format json
 ```

 - Run using a preset config file (presets are in `examples/`):

 ```bash
 ./pulsebench --config examples/config_simd.json
 ```

 Interpreting JSON output

 Example output includes the following keys (abridged):

 ```json
 {
   "workload": "simd",
   "threads": 4,
   "duration_seconds": 10,
   "total_batches": 1234,
   "throughput_batches_per_s": 123.4,
   "score": 2098,
   "stats": { ... },
   "histogram_bins": [ ... ]
 }
 ```

 - `throughput_batches_per_s`: measured batches per second (useful to reason about raw throughput).  
 - `score`: the compact, easy-to-read performance score derived from throughput (use this for quick comparisons).  
 - `stats`: statistical summary (mean/median/stddev/min/max in ms) and `percentiles` (50/90/99).  
 - `histogram_bins`: a 10-bin linear histogram of per-batch sample durations (from min->max).

 CSV output

 CSV contains a single-line summary. The header looks like:

 ```
 workload,threads,duration_seconds,total_batches,throughput_bps,score,mean_ms,median_ms,stddev_ms,min_ms,max_ms
 ```

 Quick visualization

 Use the included plotting helper to visualize JSON outputs (histogram + percentiles):

 ```bash
 python3 scripts/plot_results.py results.json
 ```

 Notes & caveats

 - `io` workload writes to `/tmp/pulsebench_io_test.bin` for demo purposes — remove that file if needed.  
 - The compact score is intentionally not a direct hardware metric (like IPC or FLOPS). It is a comparative score derived from measured throughput for convenient reporting. If you need an absolute, reproducible metric for publication, export `throughput_batches_per_s` and report that alongside environment details (CPU model, OS, compiler, compiler flags).

 Feedback and next steps

 If you prefer a different scoring method (linear scaling, baseline normalization, or mapping to a specific 0-1000 range), tell us which scheme you want and we’ll add a `--score-mode` flag to select the mapping. We can also add an optional `--baseline` value so scores can be normalized to a reference machine.

 Thanks for testing the beta. Please open an issue or create a PR for any problems you encounter.
